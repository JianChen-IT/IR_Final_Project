{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems\n",
    "\n",
    "Students: Irene Cantero (U151206) / Jian Chen (U150279)\n",
    "\n",
    "This notebook contains the 4 algorithms requested in the project sentence + 1 algorithm chose by us.\n",
    "\n",
    "Content:\n",
    "\n",
    "- Alternate Least Squares (ALS)\n",
    "- Adamic-Adar\n",
    "- Personalized PageRank\n",
    "- Node2Vec\n",
    "- Doc2Vec (chose by us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_engine.search_engine import SearchEngine\n",
    "import networkx as nx\n",
    "from networkx import Graph\n",
    "from sklearn.model_selection import train_test_split\n",
    "import implicit\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "from fast_pagerank import pagerank\n",
    "from fast_pagerank import pagerank_power\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import igraph\n",
    "import warnings\n",
    "import csv\n",
    "import argparse\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import node2vec\n",
    "from gensim.models import Word2Vec\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection time: 0.0\n"
     ]
    }
   ],
   "source": [
    "search_engine = SearchEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a graph where the vertices are formed by the users that retweet (users u) and the retweeted users (users v)\n",
    "#And the edge is the connection of users u to users v\n",
    "g=igraph.Graph()\n",
    "for tweet in search_engine.tweets.iterrows():\n",
    "    if str(tweet[1]['retweeted_status'])!='nan':\n",
    "        u=tweet[1]['user']['screen_name']\n",
    "        v=tweet[1]['retweeted_status']['user']['screen_name']\n",
    "        g.add_vertices(u)\n",
    "        g.add_vertices(v)\n",
    "        g.add_edges([(u,v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELCET USER:\n",
    "# If you change the user id, the recommendation of all 4 algorithms will try to satisfy that user.\n",
    "user_id=0\n",
    "user_name=g.vs[user_id]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS (Alternate Least Squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition that converts the users id to user names\n",
    "def number_to_username(alg_list: list, g) -> list:\n",
    "    alg_list_transformed=[]\n",
    "    for i in range(len(alg_list)):\n",
    "        name=g.vs[int(alg_list[i][0])]['name']\n",
    "        new_tuple=(name, alg_list[i][1])\n",
    "        alg_list_transformed.append(new_tuple)\n",
    "    return alg_list_transformed\n",
    "\n",
    "#It returns the user recommendations given a user_id\n",
    "def recommend_users(user_id: int, G:csr_matrix, g: igraph.Graph, top: int = 10) -> list:\n",
    "    ALS_recommended_users=model.recommend(user_id, G, top)\n",
    "    return ALS_recommended_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an adjency matrix from the graph\n",
    "G = g.get_adjacency().data\n",
    "#Convert the adjency matrix to csr_matrix, which is the variable type needed for doing ALS \n",
    "G = csr_matrix(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initialize ALS model\n",
    "model = implicit.als.AlternatingLeastSquares(factors=10, iterations=5, calculate_training_loss=True)\n",
    "#Train ALS model\n",
    "model.fit(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get user ids recommendations for a user\n",
    "ALS_recomdetaion_ids=recommend_users(user_id, G, g)\n",
    "#Transform the user ids to their repective name\n",
    "ALS_recomdetaion_names=number_to_username(ALS_recomdetaion_ids, g)\n",
    "print(f\"Recomendations for user {user_name} with id {user_id} using ALS (Alternate Least Squares):\")\n",
    "ALS_recomdetaion_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adamic-Adar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of Adamic-Adar algorithm\n",
    "def get_recommendation_AA(username: int, g:igraph.Graph) -> pd.DataFrame:\n",
    "    #We only need to consider those verices at distance 2\n",
    "    #We take those users at distance 1\n",
    "    neighbors_1=set(g.neighborhood(username, order=1))\n",
    "    #We take those users at distance 1 & 2\n",
    "    neighbors_2=set(g.neighborhood(username, order=2))\n",
    "    #We take only those nodes that are at distance 2\n",
    "    neighbors_only_order_2=list(neighbors_2 - neighbors_1)\n",
    "    \n",
    "    #Initialize dataframe with the user we want to recommend to as the column, and their 2-distance neighbors as indexes\n",
    "    adamic_adar_data=pd.DataFrame(columns=[username], index=neighbors_only_order_2)\n",
    "    #For every 2-distance users compute AA(x,y)\n",
    "    for user_y in neighbors_only_order_2:\n",
    "        if username!=user_y:\n",
    "            #Get neighbors for the two nodes\n",
    "            x_neighbors=set(g.neighbors(username))\n",
    "            y_neighbors=set(g.neighborhood(user_y))\n",
    "        #Get only those nodes that are neighbors of both nodes\n",
    "        same_neighbors=x_neighbors&y_neighbors\n",
    "        aa_val=0\n",
    "        #Compute the Adamic-Avar value and add it to the dataframe\n",
    "        for n in same_neighbors:\n",
    "            num_neighbors=len(g.neighbors(n))\n",
    "            aa_val+=(1/math.log(num_neighbors,10))\n",
    "        adamic_adar_data[username][user_y]=aa_val\n",
    "    #Sort values and return the top 10 recommendations\n",
    "    top_n_recommendations_aa=adamic_adar_data[username].sort_values(ascending=False)\n",
    "    aa_final_recommendation=pd.DataFrame(top_n_recommendations_aa)\n",
    "    return aa_final_recommendation.head(10)\n",
    "\n",
    "#Transform user ids to user names\n",
    "def AA_num_to_name(dataset: pd.DataFrame, g: igraph.Graph)->pd.DataFrame:\n",
    "    old_indices=list(dataset.index)\n",
    "    new_indices=[]\n",
    "    main_user_id=dataset.columns[0]\n",
    "    main_user_name=g.vs[main_user_id]['name']\n",
    "    for user_id in old_indices:\n",
    "        name=g.vs[user_id]['name']\n",
    "        new_indices.append(name)\n",
    "    new_dataset=pd.DataFrame(dataset.values, columns=[main_user_name], index=new_indices)\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Get recommended users ids for the requested user \n",
    "AA_recommendation_ids=get_recommendation_AA(user_id, g)\n",
    "#Transform the ids to usernames\n",
    "AA_recommendation_names=AA_num_to_name(AA_recommendation_ids, g)\n",
    "print(f\"Recomendations for user {user_name} with id {user_id} using Adamic-Adar:\")\n",
    "AA_recommendation_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform user ids to user names\n",
    "def pagerank_clearer(pagerank_values: list, g: igraph.Graph) -> list:\n",
    "    pagerank=[]\n",
    "    for i in range(len(pagerank_values)):\n",
    "            user=pagerank_values[i][0]\n",
    "            name=g.vs[user]['name']\n",
    "            val=float(pagerank_values[i][1])\n",
    "            pagerank.append((name, val))\n",
    "    return pagerank\n",
    "\n",
    "#From the score obtained from PageRank algorithm, get top user ids with higher score and that are 2-distance neighbors\n",
    "def top_10_ids(pagerank_result: list, user_id: int)->list:\n",
    "    pagerank_with_ids=[]\n",
    "    \n",
    "    #We only need to consider those verices at distance 2\n",
    "    #We take those users at distance 1\n",
    "    neighbors_1=set(g.neighborhood(user_id, order=1))\n",
    "    #We take those users at distance 1 & 2\n",
    "    neighbors_2=set(g.neighborhood(user_id, order=2))\n",
    "    #We take only those nodes that are at distance 2\n",
    "    neighbors_only_order_2=list(neighbors_2 - neighbors_1)\n",
    "    \n",
    "    for i in range(len(pagerank_result)):\n",
    "        if i!=user_id and i in neighbors_only_order_2:\n",
    "            pagerank_with_ids.append([i, pagerank_result[i]])\n",
    "    pagerank_with_ids.sort(key = lambda x: x[1], reverse=True)\n",
    "    pagerank_top_10_ids=pagerank_with_ids[0:10]\n",
    "    return pagerank_top_10_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Get PageRank scores for each user given the initial node/user\n",
    "PageRank_recommendations=g.personalized_pagerank(directed=True, reset_vertices=user_id)\n",
    "#Get top users ids with higher score and that are 2-distance neighbor \n",
    "PageRank_recommendation_ids=top_10_ids(PageRank_recommendations, user_id)\n",
    "#Transform the ids to usernames\n",
    "PageRank_recommendation_names=pagerank_clearer(PageRank_recommendation_ids, g)\n",
    "print(f\"Recomendations for user {user_name} with id {user_id} using Personalized PageRank:\")\n",
    "PageRank_recommendation_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to apply Node2vec, we need to convert the igraph to a networkx graph\n",
    "A = g.get_edgelist()\n",
    "graph = nx.Graph(A) # In case your graph is directed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Node2vec model\n",
    "node2vec = node2vec.Node2Vec(graph, dimensions=64, walk_length=2, num_walks=200, workers=4) \n",
    "#Train Node2vec model\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a list of the recommended users with their respective scores of the initial/main node\n",
    "user_id_str=str(user_id)\n",
    "node2vec_recommendation_ids=model.wv.most_similar(user_id_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Transform user ids to user names\n",
    "node2vec_recommendation_names=number_to_username(node2vec_recommendation_ids, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Recomendations for user {user_name} with id {user_id} using Node2vec:\")\n",
    "node2vec_recommendation_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOC2VEC\n",
    "\n",
    "We wanted to exploit the text feature using Doc2Vec to recommend tweets based on the content of it. This was an opportunity for us to compare Word2Vec and Doc2Vec and see the pros and cons of both of them. So in this case, the nodes are not users but tweets, and based in the content a a tweet, we propose 10 tweets (Doc2Vec uses cosine similarity to do the recommendation, as well as Word2Vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Initialization of Doc2Vec\n",
    "def initialize_doc2vec(tweets: pd.DataFrame):\n",
    "    tweets_ = []\n",
    "    i = 0\n",
    "    # Preparing the data to be put in Doc2Vec\n",
    "    for line in tweets[\"original_text\"]:\n",
    "        tokens = simple_preprocess(line)\n",
    "        tweets_.append(TaggedDocument(tokens, [i]))\n",
    "        i += 1\n",
    "    #Train the data and return\n",
    "    d2v_model = Doc2Vec(documents=tweets_, vector_size=100, window=2, min_count=1, negative = 0, workers=4)\n",
    "    return d2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = initialize_doc2vec(search_engine.tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the top 10 similar tweets related to a given tweet\n",
    "def tweet2vec_top10(input_: str, tweets: pd.DataFrame):\n",
    "    #Infer the vector to be able to let Doc2Vec do Cosine similarity\n",
    "    embedded_input= d2v_model.infer_vector(input_.split())\n",
    "    #Perform cosine similarity and return the top 10\n",
    "    recommendations = d2v_model.docvecs.most_similar([embedded_input])\n",
    "    #Preparing a DataFrame to return the top 10, by putting in the first row the input\n",
    "    recommended_tweets = [input_]\n",
    "    cosine_similarities = [1]\n",
    "    results = pd.DataFrame(columns = [\"Tweet\", \"Similarity\"])\n",
    "    # Preparing the list of the top 10\n",
    "    for position, cos_similarity in recommendations:\n",
    "        recommended_tweets.append(tweets[\"original_text\"][position])\n",
    "        cosine_similarities.append(str(cos_similarity))\n",
    "    # Putting the lists in the DataFrame and returning\n",
    "    results[\"Tweet\"] = recommended_tweets\n",
    "    results[\"Similarity\"] = cosine_similarities\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "test = tweet2vec_top10(search_engine.tweets[\"original_text\"][0], search_engine.tweets)\n",
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
