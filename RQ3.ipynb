{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_engine.search_engine import SearchEngine\n",
    "import networkx as nx\n",
    "from networkx import Graph\n",
    "from sklearn.model_selection import train_test_split\n",
    "import implicit\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "from fast_pagerank import pagerank\n",
    "from fast_pagerank import pagerank_power\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import igraph\n",
    "import warnings\n",
    "import csv\n",
    "import argparse\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import node2vec\n",
    "from gensim.models import Word2Vec\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection time: 0.0\n"
     ]
    }
   ],
   "source": [
    "search_engine = SearchEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a graph where the vertices are formed by the users that retweet (users u) and the retweeted users (users v)\n",
    "#And the edge is the connection of users u to users v\n",
    "g=igraph.Graph()\n",
    "for tweet in search_engine.tweets.iterrows():\n",
    "    if str(tweet[1]['retweeted_status'])!='nan':\n",
    "        u=tweet[1]['user']['screen_name']\n",
    "        v=tweet[1]['retweeted_status']['user']['screen_name']\n",
    "        g.add_vertices(u)\n",
    "        g.add_vertices(v)\n",
    "        g.add_edges([(u,v)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Recomender Systems*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELCET USER ID:\n",
    "user_id=0\n",
    "user_name=g.vs[user_id]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These two definitions are taken from the lab sessions of the subject\n",
    "def find_nodes_at_distance_2(graph):\n",
    "\n",
    "    all_potential_recommendations = set()\n",
    "    \n",
    "    for n1 in graph.vs:\n",
    "        \n",
    "        # all the nodes at distance 1\n",
    "        nodes_at_most_distant_1 = set(graph.neighborhood(n1, order=1))\n",
    "\n",
    "        # all the nodes at distance 1 and distance 2\n",
    "\n",
    "        nodes_at_most_distant_2 = set(graph.neighborhood(n1, order=2))\n",
    "\n",
    "        # only the nodes at distance 2\n",
    "        only_nodes_at_distance_2 = nodes_at_most_distant_2 - nodes_at_most_distant_1\n",
    "        \n",
    "        \n",
    "        # check if empty set\n",
    "        if len(nodes_at_most_distant_2) > 0:\n",
    "            for n2 in nodes_at_most_distant_2:\n",
    "                \n",
    "                # since n1 is an igraph vertex object, we need to extract the id\n",
    "                n1_index = n1.index\n",
    "                if n1_index!=n2:\n",
    "                    all_potential_recommendations.add((n1_index, n2))\n",
    "            \n",
    "    return all_potential_recommendations\n",
    "\n",
    "def predict_ALS(testset, model):\n",
    "    # initialize the empty list\n",
    "    all_predictions = []\n",
    "\n",
    "    # scroll the obs\n",
    "    for n1,n2, w in testset:\n",
    "        \n",
    "        # take here the low-dimensional vectors returned by the matrix factorization\n",
    "        print(n1)\n",
    "        array_n1 = model.user_factors[n1,:]\n",
    "        array_n2 = model.item_factors[n2,:]\n",
    "\n",
    "        # multiplying these vectors we generate an approximation for the edge score\n",
    "        one_p = np.dot(array_n1, array_n2)\n",
    "\n",
    "        all_predictions.append(one_p)\n",
    "        \n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code taken from the lab sessions of the subject\n",
    "edges_max_distance_2=find_nodes_at_distance_2(g)\n",
    "\n",
    "#¬†fraction of edges to select as test-set\n",
    "p = 0.2\n",
    "\n",
    "# graphsize\n",
    "N = len(g.es)\n",
    "\n",
    "# idxs of all the edges\n",
    "all_idxs = range(N)\n",
    "\n",
    "# sample idxs of edges through the function \"choice\"\n",
    "test_idxs = np.random.choice(a=all_idxs, size=int(p*N),replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS (Alternate Least Squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition that converts the users id to user names\n",
    "def number_to_username(alg_list: list, g) -> list:\n",
    "    alg_list_transformed=[]\n",
    "    for i in range(len(alg_list)):\n",
    "        name=g.vs[int(alg_list[i][0])]['name']\n",
    "        new_tuple=(name, alg_list[i][1])\n",
    "        alg_list_transformed.append(new_tuple)\n",
    "    return alg_list_transformed\n",
    "\n",
    "#It returns the user recommendations given a user_id\n",
    "def recommend_users(user_id: int, G:csr_matrix, g: igraph.Graph, top: int = 10) -> list:\n",
    "    ALS_recommended_users=model.recommend(user_id, G, top)\n",
    "    #ALS_recommended_users_with_names=number_to_username(ALS_recommended_users, g)\n",
    "    return ALS_recommended_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code taken from the lab sessions of the subject\n",
    "ground_truth = set()\n",
    "trainset = set()\n",
    "for idx, one_edge in enumerate(g.es):\n",
    "    # take n1 and n2 idx from one_edge, that is an igraph edge *object*\n",
    "    n1 = one_edge.source\n",
    "    n2 = one_edge.target\n",
    "    if idx in test_idxs:\n",
    "        ground_truth.add((n1, n2, 1))\n",
    "    else:\n",
    "        trainset.add((n1, n2, 1))\n",
    "        \n",
    "for rec in edges_max_distance_2:\n",
    "    \n",
    "    # add to ground truth also the potential nodes\n",
    "    n1 = rec[0]\n",
    "    n2 = rec[1]\n",
    "    \n",
    "    ground_truth.add((n1,n2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an adjency matrix from the graph\n",
    "G = g.get_adjacency().data\n",
    "#Convert the adjency matrix to csr_matrix, which is the variable type needed for doing ALS \n",
    "G = csr_matrix(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initialize ALS model\n",
    "model = implicit.als.AlternatingLeastSquares(factors=10, iterations=5, calculate_training_loss=True)\n",
    "#Train ALS model\n",
    "model.fit(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get user ids recommendations for a user\n",
    "ALS_recomdetaion_ids=recommend_users(user_id, G, g)\n",
    "#Transform the user ids to their repective name\n",
    "ALS_recomdetaion_names=number_to_username(ALS_recomdetaion_ids, g)\n",
    "print(f\"Recomendations for user {user_name} with id {user_id} using ALS (Alternate Least Squares):\")\n",
    "ALS_recomdetaion_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adamic-Adar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of Adamic-Adar algorithm\n",
    "def get_recommendation_AA(username: int, g:igraph.Graph) -> pd.DataFrame:\n",
    "    #We only need to consider those verices at distance 2\n",
    "    #We take those users at distance 1\n",
    "    neighbors_1=set(g.neighborhood(username, order=1))\n",
    "    #We take those users at distance 1 & 2\n",
    "    neighbors_2=set(g.neighborhood(username, order=2))\n",
    "    #We take only those nodes that are at distance 2\n",
    "    neighbors_only_order_2=list(neighbors_2 - neighbors_1)\n",
    "    \n",
    "    #Initialize dataframe with the user we want to recommend to as the column, and their 2-distance neighbors as indexes\n",
    "    adamic_adar_data=pd.DataFrame(columns=[username], index=neighbors_only_order_2)\n",
    "    #For every 2-distance users compute AA(x,y)\n",
    "    for user_y in neighbors_only_order_2:\n",
    "        if username!=user_y:\n",
    "            #Get neighbors for the two nodes\n",
    "            x_neighbors=set(g.neighbors(username))\n",
    "            y_neighbors=set(g.neighborhood(user_y))\n",
    "        #Get only those nodes that are neighbors of both nodes\n",
    "        same_neighbors=x_neighbors&y_neighbors\n",
    "        aa_val=0\n",
    "        #Compute the Adamic-Avar value and add it to the dataframe\n",
    "        for n in same_neighbors:\n",
    "            num_neighbors=len(g.neighbors(n))\n",
    "            aa_val+=(1/math.log(num_neighbors,10))\n",
    "        adamic_adar_data[username][user_y]=aa_val\n",
    "    #Sort values and return the top 10 recommendations\n",
    "    top_n_recommendations_aa=adamic_adar_data[username].sort_values(ascending=False)\n",
    "    aa_final_recommendation=pd.DataFrame(top_n_recommendations_aa)\n",
    "    return aa_final_recommendation.head(10)\n",
    "\n",
    "#Transform user ids to user names\n",
    "def AA_num_to_name(dataset: pd.DataFrame, g: igraph.Graph)->pd.DataFrame:\n",
    "    old_indices=list(dataset.index)\n",
    "    new_indices=[]\n",
    "    main_user_id=dataset.columns[0]\n",
    "    main_user_name=g.vs[main_user_id]['name']\n",
    "    for user_id in old_indices:\n",
    "        name=g.vs[user_id]['name']\n",
    "        new_indices.append(name)\n",
    "    new_dataset=pd.DataFrame(dataset.values, columns=[main_user_name], index=new_indices)\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Liensevi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N30Foll0w</th>\n",
       "      <td>1.43068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FloydStad</th>\n",
       "      <td>1.43068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kasmouse</th>\n",
       "      <td>1.43068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dennis91842840</th>\n",
       "      <td>1.43068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Liensevi\n",
       "N30Foll0w       1.43068\n",
       "FloydStad       1.43068\n",
       "kasmouse        1.43068\n",
       "dennis91842840  1.43068"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get recommended users ids for the requested user \n",
    "AA_recommendation_ids=get_recommendation_AA(user_id, g)\n",
    "#Transform the ids to usernames\n",
    "AA_recommendation_names=AA_num_to_name(AA_recommendation_ids, g)\n",
    "print(f\"Recomendations for user {user_name} with id {user_id} using Adamic-Adar:\")\n",
    "AA_recommendation_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform user ids to user names\n",
    "def pagerank_clearer(pagerank_values: list, g: igraph.Graph) -> list:\n",
    "    pagerank=[]\n",
    "    for i in range(len(pagerank_values)):\n",
    "            user=pagerank_values[i][0]\n",
    "            name=g.vs[user]['name']\n",
    "            val=float(pagerank_values[i][1])\n",
    "            pagerank.append((name, val))\n",
    "    return pagerank\n",
    "\n",
    "#From the score obtained from PageRank algorithm, get top user ids with higher score and that are 2-distance neighbors\n",
    "def top_10_ids(pagerank_result: list, user_id: int)->list:\n",
    "    pagerank_with_ids=[]\n",
    "    \n",
    "    #We only need to consider those verices at distance 2\n",
    "    #We take those users at distance 1\n",
    "    neighbors_1=set(g.neighborhood(user_id, order=1))\n",
    "    #We take those users at distance 1 & 2\n",
    "    neighbors_2=set(g.neighborhood(user_id, order=2))\n",
    "    #We take only those nodes that are at distance 2\n",
    "    neighbors_only_order_2=list(neighbors_2 - neighbors_1)\n",
    "    \n",
    "    for i in range(len(pagerank_result)):\n",
    "        if i!=user_id and i in neighbors_only_order_2:\n",
    "            pagerank_with_ids.append([i, pagerank_result[i]])\n",
    "    pagerank_with_ids.sort(key = lambda x: x[1], reverse=True)\n",
    "    pagerank_top_10_ids=pagerank_with_ids[0:10]\n",
    "    return pagerank_top_10_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kasmouse', 0.0807136386394075),\n",
       " ('dennis91842840', 0.06518142272913913),\n",
       " ('N30Foll0w', 0.06518142272913913),\n",
       " ('FloydStad', 0.06518142272913913)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get PageRank scores for each user given the initial node/user\n",
    "PageRank_recommendations=g.personalized_pagerank(directed=True, reset_vertices=user_id)\n",
    "#Get top users ids with higher score and that are 2-distance neighbor \n",
    "PageRank_recommendation_ids=top_10_ids(PageRank_recommendations, user_id)\n",
    "#Transform the ids to usernames\n",
    "PageRank_recommendation_names=pagerank_clearer(PageRank_recommendation_ids, g)\n",
    "print(f\"Recomendations for user {user_name} with id {user_id} using Personalized PageRank:\")\n",
    "PageRank_recommendation_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to apply Node2vec, we need to convert the igraph to a networkx graph\n",
    "A = g.get_edgelist()\n",
    "graph = nx.Graph(A) # In case your graph is directed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc1f8d8b5cc4c069611ee1799725249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Computing transition probabilities', max=7377.0, style=Pr‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Initialize Node2vec model\n",
    "node2vec = node2vec.Node2Vec(graph, dimensions=64, walk_length=2, num_walks=200, workers=4) \n",
    "#Train Node2vec model\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a list of the recommended users with their respective scores of the initial/main node\n",
    "user_id_str=str(user_id)\n",
    "node2vec_recommendation_ids=model.wv.most_similar(user_id_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Transform user ids to user names\n",
    "node2vec_recommendation_names=number_to_username(node2vec_recommendation_ids, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('FloydStad', 0.9992849826812744),\n",
       " ('N30Foll0w', 0.9992270469665527),\n",
       " ('dennis91842840', 0.9991434216499329),\n",
       " ('kasmouse', 0.9295623302459717),\n",
       " ('Alexbobby2262C', 0.6820331811904907),\n",
       " ('ZeitlowGal', 0.6716615557670593),\n",
       " ('BJohonson', 0.6669320464134216),\n",
       " ('raslady1', 0.6645399928092957),\n",
       " ('MAGA777999', 0.664025068283081),\n",
       " ('StevenS82419733', 0.6636382341384888)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Recomendations for user {user_name} with id {user_id} using Node2vec:\")\n",
    "node2vec_recommendation_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOC2VEC\n",
    "\n",
    "We wanted to exploit the text feature using Doc2Vec to recommend tweets based on the content of it. This was an opportunity for us to compare Word2Vec and Doc2Vec and see the pros and cons of both of them. So in this case, the nodes are not users but tweets, and based in the content a a tweet, we propose 10 tweets (Doc2Vec uses cosine similarity to do the recommendation, as well as Word2Vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Initialization of Doc2Vec\n",
    "def initialize_doc2vec(tweets: pd.DataFrame):\n",
    "    tweets_ = []\n",
    "    i = 0\n",
    "    # Preparing the data to be put in Doc2Vec\n",
    "    for line in tweets[\"original_text\"]:\n",
    "        tokens = simple_preprocess(line)\n",
    "        tweets_.append(TaggedDocument(tokens, [i]))\n",
    "        i += 1\n",
    "    #Train the data and return\n",
    "    d2v_model = Doc2Vec(documents=tweets_, vector_size=100, window=2, min_count=1, negative = 0, workers=4)\n",
    "    return d2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = initialize_doc2vec(search_engine.tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the top 10 similar tweets related to a given tweet\n",
    "def tweet2vec_top10(input_: str, tweets: pd.DataFrame):\n",
    "    #Infer the vector to be able to let Doc2Vec do Cosine similarity\n",
    "    embedded_input= d2v_model.infer_vector(input_.split())\n",
    "    #Perform cosine similarity and return the top 10\n",
    "    recommendations = d2v_model.docvecs.most_similar([embedded_input])\n",
    "    #Preparing a DataFrame to return the top 10, by putting in the first row the input\n",
    "    recommended_tweets = [input_]\n",
    "    cosine_similarities = [1]\n",
    "    results = pd.DataFrame(columns = [\"Tweet\", \"Similarity\"])\n",
    "    # Preparing the list of the top 10\n",
    "    for position, cos_similarity in recommendations:\n",
    "        recommended_tweets.append(tweets[\"original_text\"][position])\n",
    "        cosine_similarities.append(str(cos_similarity))\n",
    "    # Putting the lists in the DataFrame and returning\n",
    "    results[\"Tweet\"] = recommended_tweets\n",
    "    results[\"Similarity\"] = cosine_similarities\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @DGPurser: December 7, 1941, a day which will live in infamy. Japan woke a sleeping tiger. So has the left - cheaters, the deep state,‚Ä¶</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Boyd_2650: üî¥üîµThis election is NOT OVER &amp;amp; Joe Biden HAS NOT WON! And we patriots CANNOT STAND BY &amp;amp; LET EVIL LIBS &amp;amp; FOREIGN COUNTRIES TRY‚Ä¶</td>\n",
       "      <td>0.36826881766319275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @davidfrum: Newt Gingrich's former seat in US House of Representatives is now held by a black woman who advocates stricter gun laws. Gin‚Ä¶</td>\n",
       "      <td>0.3492737412452698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @AdamParkhomenko: It really is fucking incredible the way trump can get caught committing a crime on Saturday and the whole damn city of‚Ä¶</td>\n",
       "      <td>0.3406864404678345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is what Trump meant when he said Build the Wall</td>\n",
       "      <td>0.3288267254829407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@CNN TRUMP WON TRUMP WON TRUMP WON TRUMP WON FOUR MORE YEARS FOUR MORE YEARS ‚ù§Ô∏èüòé‚ù§Ô∏èüòé‚ù§Ô∏èüòé‚ù§Ô∏èüòé‚ù§Ô∏èüòé‚ù§Ô∏èüòé‚ù§Ô∏è</td>\n",
       "      <td>0.3229094445705414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @SidneyPowell1: And BIG TECH &amp;amp; #Facebook #Google #Twitter are all into suppressing our freedom of speech to challenge this outrageous #E‚Ä¶</td>\n",
       "      <td>0.31539732217788696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @TomFitton: ELECTION CRISIS: Are @sendavidperdue, @KLoeffler and other Senators prepared to object to Electoral College electors from st‚Ä¶</td>\n",
       "      <td>0.31435340642929077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @KelemenCari: Judge asks Powell if Trump wins Georgia, can he win the election. Powell answers \"Yes, he can.\" Fraud cannot be allowed to‚Ä¶</td>\n",
       "      <td>0.3082136809825897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @thehill: Over 1,500 attorneys sign letter slamming Trump legal team's efforts to overturn election results  http‚Ä¶</td>\n",
       "      <td>0.30532968044281006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RT @RebbeSMZ: Guess we're not doing an in person bar mitzvah in June.</td>\n",
       "      <td>0.2909066677093506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                       Tweet  \\\n",
       "0   RT @DGPurser: December 7, 1941, a day which will live in infamy. Japan woke a sleeping tiger. So has the left - cheaters, the deep state,‚Ä¶                 \n",
       "1   RT @Boyd_2650: üî¥üîµThis election is NOT OVER &amp; Joe Biden HAS NOT WON! And we patriots CANNOT STAND BY &amp; LET EVIL LIBS &amp; FOREIGN COUNTRIES TRY‚Ä¶   \n",
       "2   RT @davidfrum: Newt Gingrich's former seat in US House of Representatives is now held by a black woman who advocates stricter gun laws. Gin‚Ä¶               \n",
       "3   RT @AdamParkhomenko: It really is fucking incredible the way trump can get caught committing a crime on Saturday and the whole damn city of‚Ä¶               \n",
       "4   This is what Trump meant when he said Build the Wall                                                                                                       \n",
       "5   @CNN TRUMP WON TRUMP WON TRUMP WON TRUMP WON FOUR MORE YEARS FOUR MORE YEARS ‚ù§Ô∏èüòé‚ù§Ô∏èüòé‚ù§Ô∏èüòé‚ù§Ô∏èüòé‚ù§Ô∏èüòé‚ù§Ô∏èüòé‚ù§Ô∏è                                                          \n",
       "6   RT @SidneyPowell1: And BIG TECH &amp; #Facebook #Google #Twitter are all into suppressing our freedom of speech to challenge this outrageous #E‚Ä¶           \n",
       "7   RT @TomFitton: ELECTION CRISIS: Are @sendavidperdue, @KLoeffler and other Senators prepared to object to Electoral College electors from st‚Ä¶               \n",
       "8   RT @KelemenCari: Judge asks Powell if Trump wins Georgia, can he win the election. Powell answers \"Yes, he can.\" Fraud cannot be allowed to‚Ä¶               \n",
       "9   RT @thehill: Over 1,500 attorneys sign letter slamming Trump legal team's efforts to overturn election results  http‚Ä¶                                      \n",
       "10  RT @RebbeSMZ: Guess we're not doing an in person bar mitzvah in June.                                                                                      \n",
       "\n",
       "             Similarity  \n",
       "0   1                    \n",
       "1   0.36826881766319275  \n",
       "2   0.3492737412452698   \n",
       "3   0.3406864404678345   \n",
       "4   0.3288267254829407   \n",
       "5   0.3229094445705414   \n",
       "6   0.31539732217788696  \n",
       "7   0.31435340642929077  \n",
       "8   0.3082136809825897   \n",
       "9   0.30532968044281006  \n",
       "10  0.2909066677093506   "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "test = tweet2vec_top10(search_engine.tweets[\"original_text\"][0], search_engine.tweets)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
