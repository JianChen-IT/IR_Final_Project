{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval and Web Analytics: Deep Learning for Search\n",
    "\n",
    "## Query Expansion Synonyms: word2Vec VS FastText\n",
    "\n",
    "\n",
    "- One of the most frequent issues that prevent matching is the fact that people can express a concept in multiple different ways.\n",
    "- **Synonyms** are words that differ in spelling and pronunciation, but that have the same or a very close meaning. \n",
    "- In information retrieval, it’s common to use synonyms to decorate text in order to increase the probability that an appropriate query will match. \n",
    "\n",
    "\n",
    "- We will implement query expansion through synonyms generation at query time only (not at index time) using (and comparing) two different word embedding approaches:\n",
    "    - word2vec\n",
    "    - FastText\n",
    "\n",
    "**A drawback of word2vec** is that it is not able to represent words that do not appear in the training dataset.\n",
    "\n",
    "**FastText is an extension to Word2Vec** proposed by Facebook in 2016. Instead of feeding individual words into the Neural Network, **FastText breaks words into several n-grams**. \n",
    "After training the Neural Network, we will have word embeddings for all the n-grams given the training dataset. Rare words (or words that are not in the training set) can now be properly represented since it is highly likely that some of their n-grams also appears in other words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with a subset of the COVID-19 [Open Research Dataset (CORD-19)](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) pubblished on kaggle. CORD-19 is a resource of over 200,000 scholarly articles, including over 100,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. \n",
    "\n",
    "We will issue some queries and we want to get the text that is related to the submitted query.\n",
    "\n",
    "The dataset has already been preprocessed and stored in a csv file (```'inputs/biorxiv_clean.csv'```) for each paper we have:\n",
    "- paper_id\n",
    "- title\n",
    "- authors\n",
    "- text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Carlos\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "import nltk\n",
    "nltk.download('punkt') # used in sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus():\n",
    "    cols = ['paper_id', 'title', 'authors', 'text']\n",
    "    \n",
    "    biorxiv_clean = pd.read_csv('inputs/biorxiv_clean.csv', na_filter=False, usecols=cols)\n",
    "    \n",
    "    # 'paper_id' is used as an index column\n",
    "    biorxiv_clean.set_index('paper_id', inplace=True)    \n",
    "    return biorxiv_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1625, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = load_corpus()\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bbf09194127619f57b3ddf5daf684593a5831367</th>\n",
       "      <td>The Effectiveness of Targeted Quarantine for M...</td>\n",
       "      <td>Alastair Jamieson-Lane, Eric Cytrnbaum</td>\n",
       "      <td>Introduction\\n\\nCOVID-19, initially observed/d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2a21fdd15e07c89c88e8c2f6c6ab5692568876ec</th>\n",
       "      <td>Evaluation of Group Testing for SARS-CoV-2 RNA</td>\n",
       "      <td>Nasa Sinnott-Armstrong, Daniel L Klein, Brenda...</td>\n",
       "      <td>Introduction\\n\\nGroup testing was first descri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e686d1ce1540026ecb100c09f99ed091c139b92c</th>\n",
       "      <td>Why estimating population-based case fatality ...</td>\n",
       "      <td>Lucas Böttcher, Mingtao Xia, Tom Chou</td>\n",
       "      <td>\\n\\nDifferent ways of calculating mortality ra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      title  \\\n",
       "paper_id                                                                                      \n",
       "bbf09194127619f57b3ddf5daf684593a5831367  The Effectiveness of Targeted Quarantine for M...   \n",
       "2a21fdd15e07c89c88e8c2f6c6ab5692568876ec     Evaluation of Group Testing for SARS-CoV-2 RNA   \n",
       "e686d1ce1540026ecb100c09f99ed091c139b92c  Why estimating population-based case fatality ...   \n",
       "\n",
       "                                                                                    authors  \\\n",
       "paper_id                                                                                      \n",
       "bbf09194127619f57b3ddf5daf684593a5831367             Alastair Jamieson-Lane, Eric Cytrnbaum   \n",
       "2a21fdd15e07c89c88e8c2f6c6ab5692568876ec  Nasa Sinnott-Armstrong, Daniel L Klein, Brenda...   \n",
       "e686d1ce1540026ecb100c09f99ed091c139b92c              Lucas Böttcher, Mingtao Xia, Tom Chou   \n",
       "\n",
       "                                                                                       text  \n",
       "paper_id                                                                                     \n",
       "bbf09194127619f57b3ddf5daf684593a5831367  Introduction\\n\\nCOVID-19, initially observed/d...  \n",
       "2a21fdd15e07c89c88e8c2f6c6ab5692568876ec  Introduction\\n\\nGroup testing was first descri...  \n",
       "e686d1ce1540026ecb100c09f99ed091c139b92c  \\n\\nDifferent ways of calculating mortality ra...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Split data into paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full text of a paper is contained in column 'text'.\n",
    "\n",
    "Let's work at paragraph-level, in this way the search can give more detailed answers and more readable answers than full-text search.\n",
    "\n",
    "- **Split the text of the paper in paragraphs based on a simple rule: paragraphs are separated by \"\\n\", a paragraph must be at least 100 characters in length.**\n",
    "\n",
    "- We also return respective paper IDs because we want to go from a paragraph to its respective paper later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraphs(corpus, sep=\"\\n\", min_length=100, verbose=10000):\n",
    "    paragraphs, paper_ids = [], []\n",
    "    \n",
    "    for i, (paper_id, row) in enumerate(corpus.iterrows()):\n",
    "        \n",
    "        #loop over all texts\n",
    "        for text in row[\"text\"].split(sep):\n",
    "            if len(text) > min_length: #check paragraph length\n",
    "                paragraphs.append(text) #add paragrap to list of all paragraphs\n",
    "                paper_ids.append(paper_id) #add paper id to list of all paper ids\n",
    "        \n",
    "        # print progress if needed\n",
    "        if verbose > 0 and (i + 1) % verbose == 0:\n",
    "            print(f\"Progress: {i + 1}\")\n",
    "            \n",
    "    return paragraphs, paper_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15768"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs, paper_ids = list(get_paragraphs(corpus, verbose=1000))\n",
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COVID-19, initially observed/detected in Hubei province of China during December 2019, has since spread to all but a handful countries, causing (as of the time of writing) an estimated 855,000 infections and 42,000 deaths ( [8] , March 31st). COVID-19 has a basic reproductive number, R 0 , currently estimated in the region of 2.5 -3 [5] . Social distance and general quarantine measures can reduce R 0 temporarily, but not permanently. For R 0 = 3, left unchecked COVID-19 can be expected to infect more than 90% of our community, with 30% of the population infected at the epidemic peak. Even with significant quarantine measures in place the population will not reach \"herd immunity\" to this virus until 2/3 of the population has gained resistance-either through vaccination, or infection and subsequent recovery.In order to place these numbers in a concrete context, a recent survey in New Zealand indicated that the country had a total of 520 ventilator machines [7] . Given the country\\'s demographics (see table 1) , and current estimates of 1 Table 1 : Here we provide demographic data for New Zealand [2] , along with risk of ICU admission per infection for each age group [1] . Finally we give the expected number of ICU admissions, assuming 2/3 rd s of each age category becomes infected over the course of the epidemic-the minimum required to reach herd immunity.Age This is 15 times more demand than could be accommodated in the expected 4 month span of an unmitigated epidemic. The details of this calculation may vary from country to county, but the final conclusion is ubiquitous -hospitals are not prepared for this disease. Efforts to \"flatten the curve\" will need to reduce the epidemic peak not merely by a factor of two, but instead by an order of magnitude or more. Even in the most optimistic scenarios, for the most well equipped countries, such efforts must be maintained for years on end.Societal lockdown may be effective at eradicating COVID-19 locally, but when lockdown is complete a large susceptible population will remain; if the virus is later re-introduced, as expected in our globalized and interconnected world, a new epidemic is likely to occur. While buying time allows for manufacturing of new medical equipment, and further scientific investigations, such efforts can not be maintained indefinitely. For this reason it proves necessary to discuss not just what quarantine measures are needed, but also how society might return to normal, and over what time frame this can be achieved.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bbf09194127619f57b3ddf5daf684593a5831367'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Preprocess paragraphs\n",
    "\n",
    "Preprocess and tokenize paragraphs for the search engine. Please, use ```gensim.parsing.preprocessing.preprocess_string``` (notice, it is passed as input parameter) to:\n",
    "- removes all punctuation, numbers, whitespaces, and stop words.\n",
    "- tokenizes the result.\n",
    "\n",
    "**Note:** function ```get_tokens``` is a **generator**: this way we can tokenize the entire corpus on the fly, i.e. without loading the result in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(docs, preprocess=preprocess_string, verbose=10000):\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        yield preprocess(doc) # preprocess\n",
    "        \n",
    "        # print progress if needed\n",
    "        if verbose > 0 and (i + 1) % verbose == 0:\n",
    "            print(f\"Progress: {i + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 10000\n"
     ]
    }
   ],
   "source": [
    "paragraph_tokens = list(get_tokens(paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['covid',\n",
       "  'initi',\n",
       "  'observ',\n",
       "  'detect',\n",
       "  'hubei',\n",
       "  'provinc',\n",
       "  'china',\n",
       "  'decemb',\n",
       "  'spread',\n",
       "  'hand',\n",
       "  'countri',\n",
       "  'caus',\n",
       "  'time',\n",
       "  'write',\n",
       "  'estim',\n",
       "  'infect',\n",
       "  'death',\n",
       "  'march',\n",
       "  'covid',\n",
       "  'basic',\n",
       "  'reproduct',\n",
       "  'number',\n",
       "  'current',\n",
       "  'estim',\n",
       "  'region',\n",
       "  'social',\n",
       "  'distanc',\n",
       "  'gener',\n",
       "  'quarantin',\n",
       "  'measur',\n",
       "  'reduc',\n",
       "  'temporarili',\n",
       "  'perman',\n",
       "  'left',\n",
       "  'uncheck',\n",
       "  'covid',\n",
       "  'expect',\n",
       "  'infect',\n",
       "  'commun',\n",
       "  'popul',\n",
       "  'infect',\n",
       "  'epidem',\n",
       "  'peak',\n",
       "  'signific',\n",
       "  'quarantin',\n",
       "  'measur',\n",
       "  'place',\n",
       "  'popul',\n",
       "  'reach',\n",
       "  'herd',\n",
       "  'immun',\n",
       "  'viru',\n",
       "  'popul',\n",
       "  'gain',\n",
       "  'resist',\n",
       "  'vaccin',\n",
       "  'infect',\n",
       "  'subsequ',\n",
       "  'recoveri',\n",
       "  'order',\n",
       "  'place',\n",
       "  'number',\n",
       "  'concret',\n",
       "  'context',\n",
       "  'recent',\n",
       "  'survei',\n",
       "  'new',\n",
       "  'zealand',\n",
       "  'indic',\n",
       "  'countri',\n",
       "  'total',\n",
       "  'ventil',\n",
       "  'machin',\n",
       "  'given',\n",
       "  'countri',\n",
       "  'demograph',\n",
       "  'tabl',\n",
       "  'current',\n",
       "  'estim',\n",
       "  'tabl',\n",
       "  'provid',\n",
       "  'demograph',\n",
       "  'data',\n",
       "  'new',\n",
       "  'zealand',\n",
       "  'risk',\n",
       "  'icu',\n",
       "  'admiss',\n",
       "  'infect',\n",
       "  'ag',\n",
       "  'group',\n",
       "  'final',\n",
       "  'expect',\n",
       "  'number',\n",
       "  'icu',\n",
       "  'admiss',\n",
       "  'assum',\n",
       "  'ag',\n",
       "  'categori',\n",
       "  'infect',\n",
       "  'cours',\n",
       "  'epidem',\n",
       "  'minimum',\n",
       "  'requir',\n",
       "  'reach',\n",
       "  'herd',\n",
       "  'immun',\n",
       "  'ag',\n",
       "  'time',\n",
       "  'demand',\n",
       "  'accommod',\n",
       "  'expect',\n",
       "  'month',\n",
       "  'span',\n",
       "  'unmitig',\n",
       "  'epidem',\n",
       "  'detail',\n",
       "  'calcul',\n",
       "  'vari',\n",
       "  'countri',\n",
       "  'counti',\n",
       "  'final',\n",
       "  'conclus',\n",
       "  'ubiquit',\n",
       "  'hospit',\n",
       "  'prepar',\n",
       "  'diseas',\n",
       "  'effort',\n",
       "  'flatten',\n",
       "  'curv',\n",
       "  'need',\n",
       "  'reduc',\n",
       "  'epidem',\n",
       "  'peak',\n",
       "  'mere',\n",
       "  'factor',\n",
       "  'instead',\n",
       "  'order',\n",
       "  'magnitud',\n",
       "  'optimist',\n",
       "  'scenario',\n",
       "  'equip',\n",
       "  'countri',\n",
       "  'effort',\n",
       "  'maintain',\n",
       "  'year',\n",
       "  'end',\n",
       "  'societ',\n",
       "  'lockdown',\n",
       "  'effect',\n",
       "  'erad',\n",
       "  'covid',\n",
       "  'local',\n",
       "  'lockdown',\n",
       "  'complet',\n",
       "  'larg',\n",
       "  'suscept',\n",
       "  'popul',\n",
       "  'remain',\n",
       "  'viru',\n",
       "  'later',\n",
       "  'introduc',\n",
       "  'expect',\n",
       "  'global',\n",
       "  'interconnect',\n",
       "  'world',\n",
       "  'new',\n",
       "  'epidem',\n",
       "  'like',\n",
       "  'occur',\n",
       "  'bui',\n",
       "  'time',\n",
       "  'allow',\n",
       "  'manufactur',\n",
       "  'new',\n",
       "  'medic',\n",
       "  'equip',\n",
       "  'scientif',\n",
       "  'investig',\n",
       "  'effort',\n",
       "  'maintain',\n",
       "  'indefinit',\n",
       "  'reason',\n",
       "  'prove',\n",
       "  'necessari',\n",
       "  'discuss',\n",
       "  'quarantin',\n",
       "  'measur',\n",
       "  'need',\n",
       "  'societi',\n",
       "  'return',\n",
       "  'normal',\n",
       "  'time',\n",
       "  'frame',\n",
       "  'achiev']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_tokens[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Buil index with BM25\n",
    "\n",
    "Use the class ```BM25``` from Gensim to build the **paragraph search index** based on paragraphs tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.bm25 import BM25\n",
    "\n",
    "bm25 = BM25(paragraph_tokens) # constructing a paragraph search index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.Implement Search\n",
    "\n",
    "Having built the paragraph index, we now need to implement function ```get_top_n``` that retrieves the top N best matching results for a given query (which is list of key words). It does the following:\n",
    "\n",
    "- Calculate BM25 scores of all indexed documents with respect to a given query.\n",
    "- Get indices of top N scores using Numpy's efficient ```argpartition()``` function.\n",
    "- Sort retieved top N scores in descending order and return their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_top_n(bm25, query, n=10):\n",
    "    \n",
    "    #using get_tokens and tranform it from string to list of terms\n",
    "    query = query.split() # cast query from string to list\n",
    "    query = list(get_tokens(query)) # apply preprocessing\n",
    "    query = [item for sublist in query for item in sublist] # transform list of list to list\n",
    "    # score docs using a specific function of bm25\n",
    "    scores = np.array(bm25.get_scores(query))\n",
    "    \n",
    "    # get indices of top N scores\n",
    "    idx = np.argpartition(scores, -n)[-n:]\n",
    "\n",
    "    # sort top N scores and return their indices\n",
    "    # if all the scores are 0 return empty list\n",
    "    if np.sum(scores[idx]) == 0: \n",
    "        return[] \n",
    "    return idx[np.argsort(-scores[idx])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Play with the search\n",
    "\n",
    "Try some queries related to Covid-19; try also to submit the query with some typo to see the result. Example queries may be:\n",
    "- covid\n",
    "- coronavirus\n",
    "- covi (with typo)\n",
    "- etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper_id: 2f516463f332a70a46e5157c6ce1030d0abc994d\n",
      "\n",
      "Text: . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.04.05.20050245 doi: medRxiv preprint Clinical screening will decrease the prevalence of COVID-19 carriers in the test population if the probability of being asymptomatic given COVID-19 (+) is less than the proportion of the population that is asymptomatic (regardless of COVID-19 (+) or (-)). We demonstrate this by proving the following claim.Claim. The sub-population not showing any symptoms will have a lower proportion of symptomatic carriers when the proportion of people not showing any symptoms in our group is less than the estimated proportion of COVID-19 carriers who are asymptomatic.We can prove this through a couple of applications of Bayes' theorem. We refer to the event that an individual does not show symptoms as no symptoms. The event that an individual has symptoms that are signs of COVID-19 is referred to as symptoms regardless of whether they carry the disease. We denote the event of carrying the COVID-19 virus as COVID-19. Then an asymptomatic carrier is referred to as no symptoms and COVID-19. Let P (A) denote the probability of an event A occuring. Written in terms of probabilities, screening will help when P (COVID-19|no symptoms) < P (COVID-19).Re-writing the left side of the inequality, we get P (COVID-19|no symptoms) = P (COVID-19 and no symptoms) P (no symptoms) = P (no symptoms|COVID-19) P (no symptoms) P (COVID-19).3 . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.04.05.20050245 doi: medRxiv preprint Therefore it is necessary and sucient for P (no P (no symptoms) < 1, which is the same as P (no symptoms|COVID-19) < P (no symptoms). This is the probability of being asymptomatic given that they have COVID-19 for the population we are testing.\n"
     ]
    }
   ],
   "source": [
    "top_idx = None\n",
    "test_query = \"covid\"\n",
    "try:\n",
    "    top_idx = get_top_n(bm25, test_query)[0]\n",
    "    print('paper_id: {}'.format(paper_ids[top_idx]))\n",
    "    print('\\nText: {}'.format(paragraphs[top_idx]))\n",
    "    \n",
    "except:\n",
    "    print(\"No matching documents found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check which key words occur in a retrieved paragraph, I implement function \"highlight\" that highlights in red given key words (as tokens) in a given paragraph (as tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def mark(s, color='black'):\n",
    "    return \"<text style=color:{}>{}</text>\".format(color, s)\n",
    "\n",
    "def highlight(keywords, tokens, color='red'):\n",
    "    keywords = keywords.split()\n",
    "    keywords = list(get_tokens(keywords))\n",
    "    keywords = [item for sublist in keywords for item in sublist]\n",
    "    kw_set = set(keywords)\n",
    "    tokens_hl = []\n",
    "    \n",
    "    for t in tokens:\n",
    "        if t in kw_set:\n",
    "            tokens_hl.append(mark(t, color=color))\n",
    "        else:\n",
    "            tokens_hl.append(t)\n",
    "    \n",
    "    return \" \".join(tokens_hl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "intern licens avail author funder grant medrxiv licens displai preprint perpetu peer review copyright holder preprint http doi org doi medrxiv preprint clinic screen decreas preval <text style=color:red>covid</text> carrier test popul probabl asymptomat given <text style=color:red>covid</text> proport popul asymptomat regardless <text style=color:red>covid</text> demonstr prove follow claim claim sub popul show symptom lower proport symptomat carrier proport peopl show symptom group estim proport <text style=color:red>covid</text> carrier asymptomat prove coupl applic bay theorem refer event individu symptom symptom event individu symptom sign <text style=color:red>covid</text> refer symptom regardless carri diseas denot event carri <text style=color:red>covid</text> viru <text style=color:red>covid</text> asymptomat carrier refer symptom <text style=color:red>covid</text> let denot probabl event occur written term probabl screen help <text style=color:red>covid</text> symptom <text style=color:red>covid</text> write left inequ <text style=color:red>covid</text> symptom <text style=color:red>covid</text> symptom symptom symptom <text style=color:red>covid</text> symptom <text style=color:red>covid</text> intern licens avail author funder grant medrxiv licens displai preprint perpetu peer review copyright holder preprint http doi org doi medrxiv preprint necessari sucient symptom symptom <text style=color:red>covid</text> symptom probabl asymptomat given <text style=color:red>covid</text> popul test"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "par = ''\n",
    "if top_idx:\n",
    "    par = paragraph_tokens[top_idx]\n",
    "HTML(highlight(test_query, par))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You may notice that when you submit a query with a typo (for instance \"covi\") the search does not find any matching result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Expansion with Word2Vec and Fast Text\n",
    "\n",
    "Here we are going to compare **word2vec** and **FastText**. We will demonstrate as the former is able to find synonyms (so to perform query expansion) for those terms that are part of the training set. On the other hand, the latter, working with n-grams, is able to generate sysnonyms also for those terms that do not appear in the training set.\n",
    "\n",
    "We will train the two models, a Word2Vec and a fastText model using **Gensim**.\n",
    "\n",
    "Since FastText word embedding models are trained on sentences, we need to split paragraphs into sentences first. \n",
    "\n",
    "Complete the function ```get_sentences```. To split paragraphs into sentences use ```nltk.sent_tokenize```.\n",
    "\n",
    "Note: the function \"get_sentences\" is a generator but we will get the list this time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(docs, verbose=10000):\n",
    "    #loop over all docs (paragraphs in our case)\n",
    "    for i, doc in enumerate(docs):\n",
    "        \n",
    "        # use nltk.sent_tokenize to split paragraphs into sentences\n",
    "        for sentence in nltk.sent_tokenize(doc):\n",
    "            # preprocess each sentence using gensim (return string not list)\n",
    "            yield \" \".join(preprocess_string(sentence))\n",
    "            \n",
    "        # print progress if needed\n",
    "        if verbose > 0 and (i + 1) % verbose == 0:\n",
    "            print(f\"Progress: {i + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 10000\n"
     ]
    }
   ],
   "source": [
    "#split paragraphs into sentences\n",
    "sentences = list(get_sentences(paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['covid initi observ detect hubei provinc china decemb spread hand countri caus time write estim infect death march',\n",
       " 'covid basic reproduct number current estim region']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **We also split each sentence into set of words to work with word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['covid',\n",
       "  'initi',\n",
       "  'observ',\n",
       "  'detect',\n",
       "  'hubei',\n",
       "  'provinc',\n",
       "  'china',\n",
       "  'decemb',\n",
       "  'spread',\n",
       "  'hand',\n",
       "  'countri',\n",
       "  'caus',\n",
       "  'time',\n",
       "  'write',\n",
       "  'estim',\n",
       "  'infect',\n",
       "  'death',\n",
       "  'march'],\n",
       " ['covid', 'basic', 'reproduct', 'number', 'current', 'estim', 'region']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split each sentence into a list od words\n",
    "words = [s.split() for s in sentences ]\n",
    "words[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Word2Vec model\n",
    "\n",
    "Use the following parameters:\n",
    "\n",
    "- embedding dimension = 100\n",
    "- window size: 10 tokens before and 10 tokens after to get wider context\n",
    "- min_count=10, # only consider tokens with at least 10 occurrences in the corpus\n",
    "- negative=15, # negative subsampling: bigger than default to sample negative examples more\n",
    "- sg: Training algorithm: 1 for skip-gram; otherwise CBOW.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a word2Vec model  \n",
    "w2v_model = Word2Vec(sentences = words, size=100, window=10, min_count=10, negative=15, sg = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At this point we have trained a word2vec model and given a query we can find similar terms that we consider synonyms and use them to expand our original query.\n",
    "\n",
    "- As example try to expand the query \"coronavirus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-b6b3efda80e9>:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  w2v_model.most_similar(query)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('coronaviru', 0.7579174041748047),\n",
       " ('betacoronaviru', 0.7556777596473694),\n",
       " ('virus', 0.750465989112854),\n",
       " ('betacoronavirus', 0.7273703813552856),\n",
       " ('cov', 0.7027322053909302),\n",
       " ('sar', 0.6927350163459778),\n",
       " ('gorbalenya', 0.6909762024879456),\n",
       " ('coronavirida', 0.689199686050415),\n",
       " ('alphacoronaviru', 0.6876479387283325),\n",
       " ('civet', 0.6861595511436462)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'coronavirus'\n",
    "w2v_model.most_similar(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement a function ```expand_query``` that, given a query, expands it with the most n similar terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(query, wv, topn=10):\n",
    "    \n",
    "    query = preprocess_string(query)\n",
    "    expanded_query = [t for t in query] # initialize with original query. Note, it is a list\n",
    "    \n",
    "    # extend each single term of the original query and append to expanded query\n",
    "    for t in query:\n",
    "        expanded_query.extend(s for s, f in wv.most_similar(t, topn=topn))\n",
    "        \n",
    "    return expanded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covid misdiagnos sari nephropathi diseas jurisdict covd afflict advoc case swiftli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-60-84dae102cdd8>:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  expanded_query.extend(s for s, f in wv.most_similar(t, topn=topn))\n"
     ]
    }
   ],
   "source": [
    "query = 'covid'\n",
    "try:\n",
    "    expanded_query = ' '.join(expand_query(query, w2v_model))\n",
    "    print(expanded_query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - notice we are getting some \"weird term\" just because we are working on preprocessed (stemmed, etc) text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform search with expanded query using Word2Vec\n",
    " - Play with some queries - **insert also query with typos (see Covi for instance)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: covid coronavirus\n",
      "Expanded query: covid coronaviru misdiagnos sari nephropathi diseas jurisdict covd afflict advoc case swiftli renam coronavirus novel ncov betacoronaviru sar deadli cov peiri broke\n",
      "---\n",
      "\n",
      "paper_id: f4636bda5c5ae53ed6c05f5aba349a84cdb862be\n",
      "\n",
      "Text: All the databases of ICMJE-accepted platforms of clinical-trial registries [10] were considered. Search terms for Chinese Clinical Trial Registry (ChiCTR) were: \"COVID-19,\" \"2019-novel Corona Virus (2019-nCoV),\" \"Novel Coronavirus Pneumonia (NCP),\" \"Severe Acute Respiratory Infection (SARI),\" and \"Severe Acute Respiratory Syndrome -Corona Virus-2 (SARS-CoV-2).\" Search terms for the Netherlands National Trial Register were \"nCoV,\" \"Coronavirus,\" \"SARS,\" \"SARI,\" \"NCP,\" and \"COVID.\" Search terms for other databases were \"2019-nCoV OR Novel Coronavirus OR New Coronavirus OR SARS-CoV-2 OR SARI OR NCP OR Novel Coronavirus Pneumonia OR COVID-19 OR Wuhan pneumonia.\"The search was conducted on 14 February 2020. The details of inclusion criteria, exclusion criteria, study identification, date extraction, rejected/combined outcomes are described in the systematic review of protocols of clinical trials of COVID-19[11]. All rights reserved. No reuse allowed without permission. author/funder, who has granted medRxiv a license to display the preprint in perpetuity.The copyright holder for this preprint (which was not peer-reviewed) is the .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-60-84dae102cdd8>:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  expanded_query.extend(s for s, f in wv.most_similar(t, topn=topn))\n"
     ]
    }
   ],
   "source": [
    "# Play with some queries - insert also query with typos\n",
    "\n",
    "expanded_query = ''\n",
    "query = 'covid coronavirus'\n",
    "try:\n",
    "    expanded_query = ' '.join(expand_query(query, w2v_model))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "top_idx = None\n",
    "print('Original query: {}'.format(query))\n",
    "print('Expanded query: {}'.format(expanded_query))\n",
    "print('---')\n",
    "try:\n",
    "    top_idx = get_top_n(bm25, expanded_query)[0]\n",
    "    print('\\npaper_id: {}'.format(paper_ids[top_idx]))\n",
    "    print('\\nText: {}'.format(paragraphs[top_idx]))\n",
    "    \n",
    "except:\n",
    "    print(\"No matching documents found\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "databas icmj accept platform clinic trial registri consid search term chines clinic trial registri chictr <text style=color:red>covid</text> <text style=color:red>novel</text> corona viru <text style=color:red>ncov</text> <text style=color:red>novel</text> <text style=color:red>coronaviru</text> pneumonia ncp sever acut respiratori infect <text style=color:red>sari</text> sever acut respiratori syndrom corona viru <text style=color:red>sar</text> <text style=color:red>cov</text> search term netherland nation trial regist <text style=color:red>ncov</text> <text style=color:red>coronaviru</text> <text style=color:red>sar</text> <text style=color:red>sari</text> ncp <text style=color:red>covid</text> search term databas <text style=color:red>ncov</text> <text style=color:red>novel</text> <text style=color:red>coronaviru</text> new <text style=color:red>coronaviru</text> <text style=color:red>sar</text> <text style=color:red>cov</text> <text style=color:red>sari</text> ncp <text style=color:red>novel</text> <text style=color:red>coronaviru</text> pneumonia <text style=color:red>covid</text> wuhan pneumonia search conduct februari detail inclus criteria exclus criteria studi identif date extract reject combin outcom describ systemat review protocol clinic trial <text style=color:red>covid</text> right reserv reus allow permiss author funder grant medrxiv licens displai preprint perpetu copyright holder preprint peer review"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par = ''\n",
    "if top_idx:\n",
    "    par = paragraph_tokens[top_idx]\n",
    "HTML(highlight(expanded_query, par))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important!! \n",
    "\n",
    "**Try to expand a query generating a synonym of a term not seen in the training set (for instance a single term query with a typo like \"ovid\")**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"word 'covi' not in vocabulary\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-60-84dae102cdd8>:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  expanded_query.extend(s for s, f in wv.most_similar(t, topn=topn))\n"
     ]
    }
   ],
   "source": [
    "query = 'covi'\n",
    "try:\n",
    "    expanded_query = ' '.join(expand_query(query, w2v_model))\n",
    "    print(expanded_query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Note: Word2Vec is able to only generate synonyms for those terms alreay seen during training phase.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Expansion with fastText\n",
    "- Above we have already splitted the paragraphs in sentences since FastText works with sentences for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 10000\n"
     ]
    }
   ],
   "source": [
    "sentences = list(get_sentences(paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why I made the list (thus storing the results in memory) is that I am going to use it twice: for building vocabulary and then fastText training. This just saves a lot of runtime in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the fastText model**. Use the following parameters:\n",
    "- sg=1, # use skip-gram: usually gives better results\n",
    "- size=100, # embedding dimension (default)\n",
    "- window=10, # window size: 10 tokens before and 10 tokens after to get wider context\n",
    "- min_count=10, # only consider tokens with at least 10 occurrences in the corpus\n",
    "- negative=15, # negative subsampling: bigger than default to sample negative examples more\n",
    "- min_n=2, # min character n-gram\n",
    "- max_n=5 # max character n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "ft_model = FastText(\n",
    "    sg=1, # use skip-gram: usually gives better results\n",
    "    size=100, # embedding dimension (default)\n",
    "    window=10, # window size: 10 tokens before and 10 tokens after to get wider context\n",
    "    min_count=10, # only consider tokens with at least 10 occurrences in the corpus\n",
    "    negative=15, # negative subsampling: bigger than default to sample negative examples more\n",
    "    min_n=2, # min character n-gram\n",
    "    max_n=5 # max character n-gram\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a vocabulary using the in-memory list of sentences and generator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100000\n",
      "Progress: 200000\n"
     ]
    }
   ],
   "source": [
    "ft_model.build_vocab(get_tokens(sentences, verbose=100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the FastText model for 3 epochs using the in-memory list of sentences and generator of tokens. Since we use a generator to get tokens on the fly, we need to call \"train\" 3 times specifying \"epochs=1\" each time. Otherwise, the generator will go out of items after epoch 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Progress: 100000\n",
      "Progress: 200000\n",
      "Epoch 1\n",
      "Progress: 100000\n",
      "Progress: 200000\n",
      "Epoch 2\n",
      "Progress: 100000\n",
      "Progress: 200000\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    \n",
    "    ft_model.train(\n",
    "        get_tokens(sentences, verbose=100000),\n",
    "        epochs=1,\n",
    "        total_examples=ft_model.corpus_count, \n",
    "        total_words=ft_model.corpus_total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained fastText model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-69-ebca6a4d8913>:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  ft_model.most_similar(test_query)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('covd', 0.861100435256958),\n",
       " ('chinazzi', 0.804714024066925),\n",
       " ('seventi', 0.8020514845848083),\n",
       " ('seventeen', 0.8010070323944092),\n",
       " ('pediatr', 0.7969161868095398),\n",
       " ('selfreport', 0.7945656776428223),\n",
       " ('sever', 0.7945213317871094),\n",
       " ('nineteen', 0.785205602645874),\n",
       " ('clinicaltri', 0.7835091352462769),\n",
       " ('sari', 0.7827175855636597)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query = \"covid\"\n",
    "ft_model.most_similar(test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform search with query expanded using FastText\n",
    "\n",
    " - Play with some queries - **insert also query with typos (see Covi for instance)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-60-84dae102cdd8>:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  expanded_query.extend(s for s, f in wv.most_similar(t, topn=topn))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: Immune response and immunity\n",
      "Expanded query: immun respons immun immuni autoimmun immunosuppress immunocompromi immunomodulatori immunolog immunogen immunodomin immunoreact immunostain respon respond rumor congress recess correspondingli regain anymor recapitul repress immuni autoimmun immunosuppress immunocompromi immunomodulatori immunolog immunogen immunodomin immunoreact immunostain\n",
      "---\n",
      "\n",
      "paper_id: fda076d8d8b2fb52990e3db1982e4af2ff0fb08e\n",
      "\n",
      "Text: The overall analysis revealed thirteen promiscuous B-cell and T-cell epitopes that consist of four immunogenic continuous B-cell epitopes (ANYVQASEK, NYVQASEK, KSVEKPAS, and TPQQPPAQ), seven discontinuous B-cell epitopes, three immunogenic MHC-I epitopes (YVYDTRGKL, FYRQGAFEL, and FTQLVAAYL) , and three immunogenic MHC-II epitopes (FFGGKVLNF, FYRQGAFEL, and  FDYALVQHF) , that proposed to be used in multi-epitope peptide vaccine designing.Molecular docking and population coverage analysis are crucial factor in the development and refinement of epitopes selection. The epitope FYRQGAFEL had exhibited an exceptional result in terms of its broad spectrum of binding with MHC-I, MHC-II, and population coverage percentage. With universal coverage of 55.50%, 95.39% in MHC-I, MHC-II respectively, and 97.95% in combined mode. This infer a massive global population coverage, alongside it displayed the strongest binding affinity, when was docked with HLA-C*12:03 (ΔG -28.8 kcal/mol) over FTQLVAAY and YVYDTRGKL. and ΔG value of -37.35 kcal/mol when was docked with HLA-DRB1*01:01. Despite this outstanding coverage and binding score, FYRQGAFEL was not recognized as one of the abundant binders to MHC-I, a possible explanation that for its binding to one or more of the commonly occurring MHC-I alleles among global residents. The epitope FDYALVQHF also showed more negative free energy of binding (-46.25 kcal/mol) with HLA-DRB1*01:01 than FYRQGAFEL, which revealed a favored stability of the FDYALVQHF -MHC-II complex. Beside that FDYALVQHF had the abundant binding profile to MHC-II alleles and had the top population coverage of 95.38%. The core epitope FFGGKVLNF had the most dominant population coverage of 98.02% in MHC-II and 98.20% in combined mode. Even though FFGGKVLNF has the highest coverage among FYRQGAFEL and FDYALVQHF, it shows the weakest binding affinity (-31.05 kcal/mol) to HLA-DRB1*01:01 among them. FFGGKVLNF is considered a good candidate despite its weak binding affinity. Regarding MHC-I binding and population coverage, our finding has shown YVYDTRGKL that had the utmost binder and coverage of 60.93% with the highest global energy of -24.32 kcal/mol among FYRQGAFEL and FTQLVAAY. When it comes to the promising B-cell epitopes, ANYVQASEK and TPQQPPAQ had the strongest affinity toward Ig G with binding energy of -27.33 and -27.54 kcal/mol respectively. KSVEKPAS and NYVQASEK come in the second place in terms of bind affinity. Apart from the ΔG binding value, the interaction between epitope and the chosen models can also be studied by analyzing the hydrogen bond between them. As in the case of the core peptide FYRQGAFEL showed eight hydrogen bonds were present in FYRQGAFEL-HLA-DRB1*01:01 complex and ten in FYRQGAFEL-HLA-C*12:03 complex. This might contribute the difference in binding energy.Multi-epitope peptide-based vaccines are showing promising results. This emerging technology has facilitated the prevention and treatment of cancer, viral, bacterial and other diseases (95) (96) (97) (98) (99) (100) . This and other works were developed peptide vaccine against Salmonella, cholera, Mycobacterium and many other; imply the progressing of computational vaccinology approach (127, .However, this field is still in its infancy and there is dire need for further wet laboratory study. It is a widely held view the . CC-BY-ND 4.0 International license is made available under a The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It . https://doi.org/10.1101/534008 doi: bioRxiv preprint importance of Hsp70 family proteins as stand-alone virulence factors and immune response modulators since it prolongs the survival rate of mice against pulmonary Cryptococcal infection (7, 46, 66, (68) (69) (70) (71) (72) (73) (74) (75) . Several evidence of lines have suggested that many of heat shock proteins family as a potential candidate in designing of recombinant vaccine in mice models; Hsp 90 in candida, Hsp 60 in Histoplasma and Hsp 70 in Schistosoma (46, 66, (85) (86) (87) .A study conducted in Hsp 70 of Trypanosoma Cruzi found four immunodominant epitopes (TLLTIDGGI, DSLTNLRAL,TLQPVERVL and RIPKVMQLV) were assayed for their recognition by CTL of HLA-A*02:01 and T. cruzi-infected transgenic B6-A2/Kb mice, two of them (TLQPVERVL and RIPKVMQLV) were also recognized by CTL of HLA-A*02:01 Chagas disease patients, indicating that these peptides are processed and displayed as MHC-I epitopes during the natural history of T. cruzi infection (82) . An immunuinformatic study in immunoreactive mannoprotein MP88 of Cryptococcus neoformans var. grubii found three potenial MHC-I, MHC-II epitopes for each (YMAADQFCL, VSYEEWMNY, and FQQRYTGTF), (YARLLSLNA, ISYGTAMAV and INQTSYARL) correspondingly, and four promising B-cell epitopes (AYSTPA, AYSTPAS, PASSNCK, and DSAYPP) (202) .Thus, this support our findings and point toward to the fact that the development of a Cryptococcal vaccine is feasible and possible through screening the Cryptococcus neoformans's immunogenic proteins and utilize the promising antigenic epitopes in peptide vaccine designing. A therapeutic vaccine that able to prevent reactivation and be effective in the setting of established Cryptococcosis (29, 62) . There are experiments with conventional vaccines in the C. neoformans field, killed vaccines have generally been ineffective and some have enhanced infection. Live vaccines using attenuated mutants have been shown to induce stronger, longer-lasting immune responses in immunocompetent (203) (204) (205) . However, live vaccines are not safe for use in immunocompromised patients and any attempt to develop a live vaccine for cryptococcosis is likely to face significant ethical outcome. In contrast, the success of subunit and conjugate vaccines against Hepatitis B virus, Haemophilus influenza type B and Streptococcus pneumonia has shown the safety and effectiveness of this approach (206) . In patients with suppressed T-cell responses will undoubtedly suffer from reduced memory responses, rendering conventional vaccine strategies useless. Hence, implementing novel combined T-cell and B-cell vaccines that have the potential to mediate protective immunity against C. neoformans would improve the quality of life of immunocompromised patients (26) . Nonetheless, the efficacy of Cryptococcus vaccine candidate to induce protection against cryptococcosis will need to be confirmed using an immune-deficient animal model system to mimic immune suppression in human populations (76) .The findings in this study are subjected to few limitations, a limited number of validated sequences were retrieved might due to the lack of equivalent data in the literature; biases could be incorporated. Furthermore, regarding HLA allele frequencies and reference sets with population coverage, there is no predictor for (HLA-DRB5*01:01, HLA-DPA1*01 and HLA-DRB3*01:01) at IEDB population coverage tool, which might mislead the inference of coverage percentage. On the other hand, computational vaccinology approach speeds up the process of successful identification of potential peptide-vaccine candidates and greatly downsize the number of epitopes to be synthesized and analyzed for experimental assays. Therefore, there is a definite need for experimental validation for the carefully chosen vaccine . CC-BY-ND 4.0 International license is made available under a The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It . https://doi.org/10.1101/534008 doi: bioRxiv preprint candidates in vitro and in vivo to fortify their antigenic and immunogenic potentials. Additionally, further computational studies are needed to be conducted in pathogens that expressed Hsp70, as it believed to find out universal epitopes that might be overlapped with other pathogens-derived Hsp70. Finally, C. neoformans expresses a significant number of virulence agents that could help the parasite to evade and evoke host immunity. Thus, screening of new immune-proteomic factors may facilitate the future development of therapeutic interventions aimed at boosting human being immunity against cryptococcosis. Theoretically, No single epitope vaccine would provide a universal protection against all Cryptococcus neoformans var. grubii strains because of allelic polymorphism among global population, and epitopes have a different binding profile with different HLA alleles (59, 157) . Nevertheless, a complete protection would be achieved by combining multiple epitopes by targeting immunodominant regions comprising of multiple epitopes. Thus, our prime vaccine candidate was a putative ten promising epitopes (ANYVQASEK, NYVQASEK, KSVEKPAS, TPQQPPAQ, YVYDTRGKL, FYRQGAFEL, FTQLVAAYL, FFGGKVLNF, FDYALVQHF, and FINAQLVDV). Together, these epitopes are forecasted to trigger T lymphocytes, B lymphocytes, and immunological memory with overall coverage above 90%. Accordingly, our in silico vaccine is expected to be the future multi-epitope peptide vaccine with potential immunogenicity and minimum allergenicity that able to stimulate desirable immune responses against all strain of Cryptococcus neoformans var. grubii with massive global population coverage.\n"
     ]
    }
   ],
   "source": [
    "# Play with some queries - insert also query with typos (see Covi for instance)\n",
    "\n",
    "expanded_query = ''\n",
    "query = 'Immune response and immunity'\n",
    "try:\n",
    "    expanded_query = ' '.join(expand_query(query, ft_model))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "top_idx = None\n",
    "print('Original query: {}'.format(query))\n",
    "print('Expanded query: {}'.format(expanded_query))\n",
    "print('---')\n",
    "try:\n",
    "    top_idx = get_top_n(bm25, expanded_query)[0]\n",
    "    print('\\npaper_id: {}'.format(paper_ids[top_idx]))\n",
    "    print('\\nText: {}'.format(paragraphs[top_idx]))\n",
    "    \n",
    "except:\n",
    "    print(\"No matching documents found\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "overal analysi reveal thirteen promiscu cell cell epitop consist <text style=color:red>immunogen</text> continu cell epitop anyvqasek nyvqasek ksvekpa tpqqppaq seven discontinu cell epitop <text style=color:red>immunogen</text> mhc epitop yvydtrgkl fyrqgafel ftqlvaayl <text style=color:red>immunogen</text> mhc epitop ffggkvlnf fyrqgafel fdyalvqhf propos multi epitop peptid vaccin design molecular dock popul coverag analysi crucial factor develop refin epitop select epitop fyrqgafel exhibit except result term broad spectrum bind mhc mhc popul coverag percentag univers coverag mhc mhc respect combin mode infer massiv global popul coverag alongsid displai strongest bind affin dock hla kcal mol ftqlvaai yvydtrgkl valu kcal mol dock hla drb despit outstand coverag bind score fyrqgafel recogn abund binder mhc possibl explan bind commonli occur mhc allel global resid epitop fdyalvqhf show neg free energi bind kcal mol hla drb fyrqgafel reveal favor stabil fdyalvqhf mhc complex fdyalvqhf abund bind profil mhc allel popul coverag core epitop ffggkvlnf domin popul coverag mhc combin mode ffggkvlnf highest coverag fyrqgafel fdyalvqhf show weakest bind affin kcal mol hla drb ffggkvlnf consid good candid despit weak bind affin mhc bind popul coverag find shown yvydtrgkl utmost binder coverag highest global energi kcal mol fyrqgafel ftqlvaai come promis cell epitop anyvqasek tpqqppaq strongest affin bind energi kcal mol respect ksvekpa nyvqasek come second place term bind affin apart bind valu interact epitop chosen model studi analyz hydrogen bond case core peptid fyrqgafel show hydrogen bond present fyrqgafel hla drb complex fyrqgafel hla complex contribut differ bind energi multi epitop peptid base vaccin show promis result emerg technolog facilit prevent treatment cancer viral bacteri diseas work develop peptid vaccin salmonella cholera mycobacterium impli progress comput vaccinolog approach field infanc dire need wet laboratori studi wide held view intern licens avail copyright holder preprint peer review author funder http doi org doi biorxiv preprint import hsp famili protein stand virul factor <text style=color:red>immun</text> respons modul prolong surviv rate mice pulmonari cryptococc infect evid line suggest heat shock protein famili potenti candid design recombin vaccin mice model hsp candida hsp histoplasma hsp schistosoma studi conduct hsp trypanosoma cruzi <text style=color:red>immunodomin</text> epitop tlltidggi dsltnlral tlqpvervl ripkvmqlv assai recognit ctl hla cruzi infect transgen mice tlqpvervl ripkvmqlv recogn ctl hla chaga diseas patient indic peptid process displai mhc epitop natur histori cruzi infect immunuinformat studi <text style=color:red>immunoreact</text> mannoprotein cryptococcu neoforman var grubii poteni mhc mhc epitop ymaadqfcl vsyeewmni fqqrytgtf yarllslna isygtamav inqtsyarl <text style=color:red>correspondingli</text> promis cell epitop aystpa aystpa passnck dsaypp support find point fact develop cryptococc vaccin feasibl possibl screen cryptococcu neoforman <text style=color:red>immunogen</text> protein util promis antigen epitop peptid vaccin design therapeut vaccin abl prevent reactiv effect set establish cryptococcosi experi convent vaccin neoforman field kill vaccin gener ineffect enhanc infect live vaccin attenu mutant shown induc stronger longer last <text style=color:red>immun</text> respons immunocompet live vaccin safe us immunocompromis patient attempt develop live vaccin cryptococcosi like face signific ethic outcom contrast success subunit conjug vaccin hepat viru haemophilu influenza type streptococcu pneumonia shown safeti effect approach patient suppress cell respons undoubtedli suffer reduc memori respons render convent vaccin strategi useless implement novel combin cell cell vaccin potenti mediat protect <text style=color:red>immun</text> neoforman improv qualiti life immunocompromis patient nonetheless efficaci cryptococcu vaccin candid induc protect cryptococcosi need confirm <text style=color:red>immun</text> defici anim model mimic <text style=color:red>immun</text> suppress human popul find studi subject limit limit number valid sequenc retriev lack equival data literatur bias incorpor furthermor hla allel frequenc refer set popul coverag predictor hla drb hla dpa hla drb iedb popul coverag tool mislead infer coverag percentag hand comput vaccinolog approach speed process success identif potenti peptid vaccin candid greatli downsiz number epitop synthes analyz experiment assai definit need experiment valid carefulli chosen vaccin intern licens avail copyright holder preprint peer review author funder http doi org doi biorxiv preprint candid vitro vivo fortifi antigen <text style=color:red>immunogen</text> potenti addition comput studi need conduct pathogen express hsp believ univers epitop overlap pathogen deriv hsp final neoforman express signific number virul agent help parasit evad evok host <text style=color:red>immun</text> screen new <text style=color:red>immun</text> proteom factor facilit futur develop therapeut intervent aim boost human <text style=color:red>immun</text> cryptococcosi theoret singl epitop vaccin provid univers protect cryptococcu neoforman var grubii strain allel polymorph global popul epitop differ bind profil differ hla allel complet protect achiev combin multipl epitop target <text style=color:red>immunodomin</text> region compris multipl epitop prime vaccin candid put promis epitop anyvqasek nyvqasek ksvekpa tpqqppaq yvydtrgkl fyrqgafel ftqlvaayl ffggkvlnf fdyalvqhf finaqlvdv epitop forecast trigger lymphocyt lymphocyt <text style=color:red>immunolog</text> memori overal coverag accordingli silico vaccin expect futur multi epitop peptid vaccin potenti <text style=color:red>immunogen</text> minimum allergen abl stimul desir <text style=color:red>immun</text> respons strain cryptococcu neoforman var grubii massiv global popul coverag"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par = ''\n",
    "if top_idx:\n",
    "    par = paragraph_tokens[top_idx]\n",
    "HTML(highlight(expanded_query, par))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important!! \n",
    "\n",
    "**Try to expand a query generating a synonym of a term not seen in the training set (for instance a single term query with a typo like \"ovid\")**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covi covid covd seventeen seventi cot seventh draconian seven residenti nineteen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-60-84dae102cdd8>:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  expanded_query.extend(s for s, f in wv.most_similar(t, topn=topn))\n"
     ]
    }
   ],
   "source": [
    "query = 'covi'\n",
    "try:\n",
    "    expanded_query = ' '.join(expand_query(query, ft_model))\n",
    "    print(expanded_query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice how FastText model is able to generate synonyms also for those terms that were not used to train the model. As a consequence it is possible to expand the query even in case of terms not previously seen.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference:\n",
    "- https://www.kaggle.com/slavaz/simple-paragraph-search-bm25-fasttext-qe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
