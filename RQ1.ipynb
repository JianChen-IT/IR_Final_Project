{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Engine\n",
    "\n",
    "Students: Irene Cantero (U151206) / Jian Chen (U150279)\n",
    "\n",
    "All the code is stored in the folder `search_engine`. The notebook only contains the calls and some functions to do the T-SNE and the clustering.\n",
    "\n",
    "Content: \n",
    "\n",
    "- Top 10 results only using TF-IDF + Cosine similarity for 10 chosen queries\n",
    "- Top 10 results using Word2Vec + Cosine similarity for the same 10 chosen queries\n",
    "- Search with custom score G(d) + cosine similarity for a given query\n",
    "        - G(d) considers the (1/2)Tweets likes, (1/3)retweets and (1/6) replies\n",
    "- T-SNE implementation and plot using T-SNE.\n",
    "- Plot to see the optimal number of clusterings.\n",
    "- Clustering using K means and the optimal number of clusterings, showing the most common words of each cluster as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_engine.search_engine import SearchEngine\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import csv\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e572214197e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msearch_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSearchEngine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Escritorio\\UNIVERSIDAD\\Forth year\\Information retrieval\\IR_Final_Project\\search_engine\\search_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatabase_setup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# used for giving shape for tweets, query_results and query_results_with_g by removing columns, and easing the Twitter API tweet structure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m# self.tfidf = []\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Running the initialization of the Search engine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# Function that collects and calls the rest of the functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Escritorio\\UNIVERSIDAD\\Forth year\\Information retrieval\\IR_Final_Project\\search_engine\\search_engine.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;34m\"States\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             ]\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTRACKING_KEYWORDS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_async\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"en\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;31m# After collection of tweets continue with the initialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, follow, track, is_async, locations, stall_warnings, languages, encoding, filter_level)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'filter_level'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_level\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'delimited'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'length'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_async\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     def sitestream(self, follow, stall_warnings=False,\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_start\u001b[1;34m(self, is_async)\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnooze_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnooze_time_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_connect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mssl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m                 \u001b[1;31m# This is still necessary, as a SSLError can actually be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_read_loop\u001b[1;34m(self, resp)\u001b[0m\n\u001b[0;32m    349\u001b[0m             \u001b[0mnext_status_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_len\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnext_status_obj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_status_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;31m# # Note: keep-alive newlines might be inserted before each length value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36mon_data\u001b[1;34m(self, raw_data)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'in_reply_to_status_id'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;34m'delete'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Escritorio\\UNIVERSIDAD\\Forth year\\Information retrieval\\IR_Final_Project\\search_engine\\core\\tweet_collector.py\u001b[0m in \u001b[0;36mon_status\u001b[1;34m(self, status)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mruns\u001b[0m \u001b[0meach\u001b[0m \u001b[0mtime\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mbunch\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtweets\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mretrived\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstreaming\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \"\"\"\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"a+\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0mtweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\_bootlocale.py\u001b[0m in \u001b[0;36mgetpreferredencoding\u001b[1;34m(do_setlocale)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"win\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mgetpreferredencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutf8_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "search_engine = SearchEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run the search engine considering the popularity (Likes, Retweets and Replies) of each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(\"Insert your query:\\n\")\n",
    "#query = input()\n",
    "query = \"joe biden\"\n",
    "search_engine.run_g(query).query(\"score > 0\").head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code to answer RQ1b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "#chosen queries\n",
    "queries=[]\n",
    "queries.append(\"joe biden won elections\")\n",
    "queries.append(\"donald trump is the president\")\n",
    "queries.append(\"elections are a fraud\")\n",
    "queries.append(\"pennsylvania\")\n",
    "queries.append(\"trump out\")\n",
    "queries.append(\"votes fraud\")\n",
    "queries.append(\"i voted\")\n",
    "queries.append(\"georgia votes\")\n",
    "queries.append(\"trump team\")\n",
    "queries.append(\"biden team\")\n",
    "\n",
    "#NOTE: to open the tsv correctly use UTF-8\n",
    "try:\n",
    "    os.remove('other-outputs/RQ1b.tsv')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "search_engine.ranking_system.change_user_output(1) #Using TF-IDF + cosine_similarity\n",
    "# Setting the header of the TSV file\n",
    "RQ1 = open('other-outputs/RQ1b.tsv', 'a+')\n",
    "RQ1.write(\"\\tTweet\\tUsername\\tDate\\tHashtags\\tLikes\\tRetweets\\tReplies\\tUrl\\tScore\\n\")\n",
    "RQ1.write(f\"QUERY\\t{queries[0]}\\n\")\n",
    "RQ1.close()\n",
    "\n",
    "# Storing each result of each query in the TSV file\n",
    "for query in queries:\n",
    "    RQ1 = open('other-outputs/RQ1b.tsv', 'a+')\n",
    "    RQ1.write(f\"QUERY\\t{query}\\n\")\n",
    "    RQ1.close()\n",
    "    print(f\"\\nQUERY: {query.upper()}\\n\")\n",
    "    results=search_engine.run(query).query(\"score > 0\").head(20)\n",
    "    display(results)\n",
    "    results.replace('\\n',' ', regex=True).to_csv(path_or_buf='other-outputs/RQ1b.tsv', sep='\\t', header=False, mode = 'a')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code to the answer RQ1c:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine.ranking_system.change_user_input(2) #Using Word2Vec + cosine_similarity\n",
    "#chosen queries\n",
    "queries=[]\n",
    "queries.append(\"joe biden won elections\")\n",
    "queries.append(\"donald trump is the president\")\n",
    "queries.append(\"elections are a fraud\")\n",
    "queries.append(\"pennsylvania\")\n",
    "queries.append(\"trump out\")#3\n",
    "queries.append(\"votes fraud\")\n",
    "queries.append(\"i voted\")\n",
    "queries.append(\"georgia votes\")\n",
    "queries.append(\"trump team\")\n",
    "queries.append(\"biden team\")\n",
    "\n",
    "#NOTE: to open the tsv correctly use UTF-8\n",
    "try:\n",
    "    os.remove('other-outputs/RQ1c.tsv')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Setting the header of the TSV file\n",
    "RQ1 = open('other-outputs/RQ1c.tsv', 'a+')\n",
    "RQ1.write(\"\\tTweet\\tUsername\\tDate\\tHashtags\\tLikes\\tRetweets\\tReplies\\tUrl\\tScore\\n\")\n",
    "RQ1.write(f\"QUERY\\t{queries[0]}\\n\")\n",
    "RQ1.close()\n",
    "\n",
    "# Storing each result of each query in the TSV file\n",
    "for query in queries:\n",
    "    RQ1 = open('other-outputs/RQ1c.tsv', 'a+')\n",
    "    RQ1.write(f\"QUERY\\t{query}\\n\")\n",
    "    RQ1.close()\n",
    "    print(f\"\\nQUERY: {query.upper()}\\n\")\n",
    "    results=search_engine.run(query).query(\"score > 0\").head(20)\n",
    "    display(results)\n",
    "    results.replace('\\n',' ', regex=True).to_csv(path_or_buf='other-outputs/RQ1c.tsv', sep='\\t', header=False, mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can you imagine a better representation than word2vec? Justify your answer.\n",
    "#(HINT - what about Doc2vec? Sentence2vec? Which are the pros and cons?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs K-means to see what are the best number of clusters, recollects the words of each cluster\n",
    "# of tweets and gives labels\n",
    "def KMeans_setup(model) -> None:\n",
    "    total_tokens=[]\n",
    "    tweets_words = []\n",
    "    # for to do word embedding using word2vec and collecting words from the tweets\n",
    "    for tweet in search_engine.tweets[\"text\"]:\n",
    "        tweet_words = {}\n",
    "        tokens = []\n",
    "        # for each word of the tweet do word emebeding, and add it to the dictionary of the tweet. If \n",
    "        # it exists already, then just add 1\n",
    "        for word in tweet.split():\n",
    "            try:\n",
    "                tokens.append(model[word])\n",
    "                if word not in tweet_words.keys():\n",
    "                    tweet_words[word] = 1\n",
    "                else:\n",
    "                    tweet_words[word]+=1\n",
    "            except:\n",
    "                pass\n",
    "        # We do the mean of the word embeddings to represent the tweet (Tweet2Vec)\n",
    "        tokens = np.mean(np.array(tokens), axis=0)\n",
    "        # For the tweets, we just append it to the tweets_words. Since the embedded tweet and tweet words are in the same order,\n",
    "        # we do not need any mapping function to make sure that the tweet words corresponds to the embedded tweet.\n",
    "        tweets_words.append(tweet_words)\n",
    "        if str(tokens) != 'nan':\n",
    "            total_tokens.append(tokens)\n",
    "    \n",
    "    # Plot the sum of square distance depending on the number of clusterings\n",
    "    K = range(1,15)\n",
    "    Sum_of_squared_distances = []\n",
    "    for k in K:\n",
    "        km = KMeans(n_clusters=k)\n",
    "        km = km.fit(total_tokens)\n",
    "        Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "    plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum_of_squared_distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "    return total_tokens, tweets_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function of the T-SNE plot, which takes as inputs the output of the function Kmeans_Setup\n",
    "# The number of clusters have been set according to the plot of Kmeans_setup.\n",
    "def tsne_plot(total_tokens, tweet_words):\n",
    "    NUM_CLUSTERS = 5\n",
    "    # T-SNE setup\n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=5000, random_state=0)\n",
    "    new_values = tsne_model.fit_transform(total_tokens)\n",
    "    \n",
    "    # K-means setup with the specified number of clusters\n",
    "    kmeans = KMeans(n_clusters=NUM_CLUSTERS)\n",
    "    kmeans = kmeans.fit(total_tokens)\n",
    "    # getting the labels to do the coloring of each node (tweet.)\n",
    "    labels = kmeans.predict(total_tokens)\n",
    "    ColorsA=plt.cm.viridis(np.linspace(0, 1, NUM_CLUSTERS),alpha=0.8)\n",
    "    \n",
    "    # Getting the results of the T-SNE model by getting the coordinates of x,y of each tweet\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "\n",
    "    clusters_common_words = []\n",
    "    \n",
    "    # Coloring each tweet node depending on the cluster it belongs. We take profit of the variable \"labels\" to do that\n",
    "    plt.figure(figsize=(10,10))\n",
    "    # For each cluster...\n",
    "    for i in range(NUM_CLUSTERS):\n",
    "        xL=[]\n",
    "        yL=[]\n",
    "        cluster_words = {}\n",
    "        # ... and for every tweet...\n",
    "        for k in range(len(x)):\n",
    "            # ...if the tweet belongs to the cluster get the coordinates, and store the words in a dictionary\n",
    "            if labels[k]==i:\n",
    "                xL.append(x[k])\n",
    "                yL.append(y[k])\n",
    "                # collecting and counting the number of words \n",
    "                for word in tweets_words[k]:\n",
    "                    if word not in cluster_words.keys():\n",
    "                        cluster_words[word] = 1\n",
    "                    else:\n",
    "                        cluster_words[word] += tweets_words[k].get(word)\n",
    "        # Associate all the words collected to a cluster\n",
    "        clusters_common_words.append(cluster_words)\n",
    "        plt.scatter(xL,yL,color=ColorsA[i])\n",
    "    \n",
    "    # Extra for loop just to show most common words of each cluster\n",
    "    for i in range(NUM_CLUSTERS):\n",
    "        print(dict(sorted(clusters_common_words[i].items(), key=lambda x: x[1], reverse=True)[:5]))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot show us the optimal number of clusters needed in our dataset. However, in this case is not very clear, because the \"elbow\" is not very well defined. We think that this is because the tweets are very similar between each other, and that leads us to have at most 2 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = search_engine.ranking_system.w2v\n",
    "total_tokens, tweets_words = KMeans_setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(total_tokens, tweets_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
